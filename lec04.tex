\begin{lecture}{4}{Uniform Hardness vs.\ Randomness}{February 10, 2022}
\label{lec:04}

\subsection*{The Upshot}

\begin{enumerate}
  \item Uniform distinguishers for the NW-PRG with hard function $f$ can be
    used to learn $f$ with $1/\poly(n)$ advantage with high probability.
  \item The small advantage can be used to bootstrap an algorithm that computes
    a well-structured problem in $\P^{\sharpp}$ with advantage. The conclusion is
    that if $\prBPP = \prP$ unless there are algorithms to solve problems in
    $\P^{\sharpp}$ on average. \comment{Is this right? I think we are missing
    a concrete hardness claim about the problem.}
  \item LLDCs and downward self-reducibility play a crucial role in the
    bootstrapping algorithm.
\end{enumerate}


\subsection{Uniform Circuits}

So far we have been studying PRGs for non-uniform circuits. As a consequence,
the results we've seen assume problems in \E are hard for non-uniform models.
In this lecture we investigate the analgous questions for uniform models of
computation.

\begin{definition}[$\eps$-PRG for uniform circuits]
  An algorithm $G$ is an \emph{$\eps$-PRG for uniform circuits generated in
  time $t(n)$} if for every Turing machine $D$ with running time $t(n)$ and
  large enough $n \in \N$
	\[
		\Pr\brac{D(1^n) \text{ is an $\eps$-distinguisher circuit }
    D_n \text{ for } G(1^n, u_{l(n)})} \leq \eps.
	\]
\end{definition}

\begin{definition}
  Let $\overline{x} = \set{\overline{x}_n}$ be a sequence of distributions over
  $\set{0, 1}^n$ and lelt $p : \N \to \N$. We say that $\overline{x}$ is
  $p$-time samplable if there is a probabilistic machine $S$ with runtime
  $p(n)$ such that $S(1^n)$ has the distribution $\overline{x}(n)$.
\end{definition}

\begin{theorem}
  Assume that for all polynomials $p(n)$ there is an $\eps$-PRG $G$ for uniform
  circuits generated in time $p(n)$ that stretches a $\log(n)$-sized seed. Then
  for all $L \in \BPP$ and poly-time samplable distributions $\overline{x}$,%
  \footnote{Think of $\overline{x}$ as an adversary (limited to polynomial
  time) trying to generate hard inputs with $\geq \eps$ probability.} there
  exists a language $L' \in \textsf{P}$ such that
	\[
		\Pr_{x \sim \overline{x}}\brac{L(x) \neq L'(x)} \leq \eps.
	\]
\end{theorem}

\begin{proof}
	Let $S$ be the sampling algorithm for $\overline{x}$, and let $M = M_L$
	be the $\BPP$ machine for $L$. We define $L'$ as: given $x$, enumerate
	over seeds $s$ of $G$, and output
	\[ \textsf{MAJ}_{s} \set{M(x, G(s)) }. \]
	Since the seed length $s$ is $O(\log n)$, the algorithm above runs in
	polynomial time, and hence $L' \in \P$.
	Further, for each $x$, we define:
	\[
		\Delta(x) = \card{
			\Pr_{r \sim \overline{u}_n}         \brac{ M(x, r) = 1}
		  - \Pr_{s \sim \overline{u}_{\ell(n)}} \brac{ M(x, s) = 1}
		  }.
	\]

	We suppose (towards a contradiction) that $L'$ differs from $L$ on more
	than an $\eps$-fraction of inputs. Since $M$ is a $\BPP$ machine for $L$
	this means that
	$\Pr_{x \sim \overline{x}_n}\brac{\Delta(x) > \eps} > \eps$. Then consider
	the algorithm $D$: on the input $1^n$, sample $x \sim \overline{x}_n$ using
	$S(1^n)$, and then output the circuit $D_x(r) = M(x, r)$.
	Then
  \begin{align*}
		\Pr\brac{D \text{ prints a circuit that } \eps \text{-distinguishes } G(1^n, u_{\ell})}
    &= \Pr\brac{\Delta(S(1^n)) > \eps}\\
    &= \Pr_{x \sim \overline{x}_n}\brac{\Delta(x) > \eps}\\
    &> \eps,
  \end{align*}
	which contradicts the fact that $G$ is an $\eps$-PRG.
\end{proof}

\begin{theorem}
	If
	$\SPACE\brac{O(n)}$ is hard for $\BPTIME\brac{2^{\eps n}}$
	then
	$\RP = \P$ ``on average''.
\end{theorem}

\newcommand{\fws}{\ensuremath{ f^{\text{ws}} }}
\begin{theorem}
	There exists a language $\fws \in \P^{\sharpp}$ such that if $\fws$
	is hard for $\BPTIME\brac{2^{\eps n}}$ then
	$\BPP = \P$ ``on average''.
\end{theorem}

\begin{proof}
	\comment{TODO.}
\end{proof}

\subsection{Learning via Uniform Distinguishers}

\comment{Restate / refer to the Nisan-Wigderson PRG here.}

\begin{definition}
  Let $f_n : \set{0, 1}^n \to \set{0, 1}.$ A probabilistic algorithm $A$
  \emph{learns} with oracle access to $f_n$ \emph{learns $f_n$ with success
  $\delta$ and advantage $\eps$} if on input $1^n$, $A$ produces a circuit
  $C_n$ such that $\Pr_x \brac{C_n(x) = f_n(x)} \geq 1/2 + \eps$.
\end{definition}

\begin{proposition}
  \label{prop:learning}
  Let $\ell = \log{n}$, let $\gamma \in (0, 1/2)$, and let $T_1, \dots, T_n$ be
  a $(\ell, \gamma \ell)$-design over $[O(\ell)]$. Finally, let $f :
  \set{0,1}^\ell \to \set{0, 1}$ and let $G^f$ be the Nisan-Wigderson PRG with
  oracle $f$ and design $T_1, \dots, T_n$. If $D$ is a uniform circuit on $n$
  inputs generated in time $\poly(n)$ that is not $(1/n)$-fooled by $G^f$, then
  there is a $\poly(n)$-time algorithm to learn $f$ with advantage $1/n^2$ and
  success $1/n$.
\end{proposition}

\begin{proof}
  By definition, if $D$ is not $(1/n)$-fooled by $G^f$, then
	\[
		\card{\Pr\brac{D(G(1^n, s)) = 1} - \Pr\brac{D(\overline{u}_n) = 1}} > 1/n.
	\]
  \comment{Should the input to $G$ really be $1^n$ above?}

  As in the hybrid argument (\comment{ref. lecture 2 here}), we define the
  distributions
  \begin{align*}
    \overline{H}_0 &= (f(x_1), \ldots, f(x_n)),\\
    \overline{H}_i &= (\overline{u}_i, f(x_{i + 1}), \ldots, f(x_n)),\\
    \overline{H}_n &= \overline{u}_n,
  \end{align*}
  where $x_i$ is the projection of the seed given to the PRG onto the
  coordinates $T_i$. Recall the (easy) lemma that there is an $i$ such that:
	\[
		\card{\Pr\brac{D(\overline{H}_{i - 1}) = 1} - \Pr\brac{D(\overline{H}_i) = 1}} > 1 / n^2.
	\]
  Roughly speaking, the learning algorithm will guess the value of this $i$ and
  exploit the fact that $D$ can differentiate the bit $f(x_i)$ from a uniformly
  random bit. More concretely, we describe the algorithm $A$:
	\begin{enumerate}
    \item Construct the a $(\ell, \gamma \ell)$-design $T_1, \dots, T_n$ in
      time $\poly(n)$ using Proposition~\ref{prop:design} from
      Lecture~\ref{lec:02}.
		\item Choose a random $i \in \brac{n}$.
		\item Choose a random $z^* \in \set{0, 1}^{\ell(n) - \ell}$. Here $s$
			(the seed of the PRG) projected to $T_i$ will be equal to $x_i$,
			and the remaining coordinates will be filled by $z^*$.
		\item Choose a random bit $\sigma$.
		\item Query the oracle for $f$ on all possible inputs
			$x_{i + 1}, \ldots , x_n$. Note that at most $\ell / 100$ bits of
			each of these $x_j$'s are unknown after fixing $z^*$, so there
			are at most $n \cdot 2^{\ell / 100}$ queries to the oracle.
		\item Choose a random $u^* \in \set{0, 1}^{i - 1}$.
		\item Output the circuit $C_n$ which simulates $D$ on
			$(u^*, \sigma, f(x_{i + 1}), \ldots , f(x_n))$ and outputs
			$\sigma$ if $D$ accepts, and $\lnot \sigma$ otherwise.
	\end{enumerate}
	It is easy\footnote{haha} to see that
	$\Pr\brac{C_n(x) = f_n(x)} = 1/2 + 1/n^2$.
\end{proof}

\begin{corollary}
  If there is a $(1/n)$-distinguisher for the NW-PRG $G^f$, then there is an
  algorithm that learns $f$ with success $1 - 1/\delta$ and advantage
  $1/(2n^2)$ that runs in time $\poly(n, 1/\delta)$.
\end{corollary}

\begin{proof}
  Let $A$ be the learning algorithm for $f$ with success $1/n$ and advantage
  $1/n^2$ promised by Proposition \ref{prop:learning}. By running $A$
  independently $n \log(1/\delta)$ times, the probability that none of the
  circuits produced has advantage $1/n^2$ is $1 - (1 - 1/n)^{n\log(1/\delta)} >
  1 - \delta$. We can estimate the acceptance probability of each of the
  $n\log(1/\delta)$ circuits to within an additive error of $1/(2n^2)$ with
  probability $1 - \delta$ by running it on $O(n^4 \log(1/\delta))$ random
  inputs. Hence if such a circuit exists, we will find it in time $\poly(n,
  1/\delta)$. Reparameterizing $\delta$ gives the result.
\end{proof}

\subsection{Bootstrapping via Downward Self-Reducibility and LLDCs}

\begin{definition}
  $f$ is \emph{downward self-reducable (DSR)} if there exists an algorithm $A$
  such that when $A$ gets $x \in \set{0, 1}^n$ and oracle access to $f$ on $n -
  1$ bits, it outputs $f(x)$ in linear time.%
  \footnote{Linear time is not critical, it can be replaced (for example) by quadratic time ... but not $\poly$-time (???).}
\end{definition}

For example, $\SAT$ and $\PERM$ are DSR.

\begin{proposition}
	There exists a function $\fws \in \P^{\sharpp}$ such that:
	\begin{enumerate}
		\item $\fws$ is DSR.
		\item For all $\ell \in \N$ the truth table of $\fws$ is a codeword
			which is:
			\begin{enumerate}
        \item Locally List Decodable from agreement $\eps(\ell) = 2^{-\ell /
          100}$ in time $\poly(1/\eps)$ with list size $\poly(1 / \eps)$.
        \item Uniquely Decodable from agreement $0.99$ in time $\poly(n)$.
			\end{enumerate}
	\end{enumerate}
\end{proposition}

\begin{theorem}
  If $\fws$ is hard for $\BPTIME\brac{2^{\eps n}}$ then for all polynomials
  $p(n)$ there is a $(1 / n)$-PRG for uniform circuits generated in time $p(n)$
  with $\log$-sized seed and polynomial running time.
\end{theorem}

\begin{proof}
	\comment{TODO.}
\end{proof}

\end{lecture}
