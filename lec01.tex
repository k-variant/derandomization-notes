\begin{lecture}{1}{The Class prBPP}{January 23, 2020}
\label{lec:01}

\subsection*{The Upshot}

\begin{enumerate}
  \item Randomness is necessary in many computational settings. Is randomness
    necessary for polynomial-time computation? There are good reasons to expect
    it is not! This is a fundamental question we will investigate in this
    course.
  \item Promise problems are a natural generalization of languages, and the
    theory of probabilistic algorithms for promise problems is richer than
    for that of languages.
  \item The class $\prBPP$ has a complete problem ($\CAPP$) under deterministic
    polynomial-time reductions. Hence to prove $\prBPP = \prP$ it suffices
    to give a deterministic polynomial-time algorithm for $\CAPP$.
  \item The class $\prBPP$ admits a time-hierarchy theorem.
  \item Probabilistic search problems can be reduced to probabilistic decision
    problems, meaning the theory we will develop around the class of decision
    problems $\prBPP$ will apply to the search analog as well.
\end{enumerate}

\subsection{Promise Problems and prBPP}

A \emph{promise problem} is a pair $\Pi = (\Pi_Y, \Pi_N) \subseteq \{0, 1\}^*
\times \{0, 1\}^*$ such that $\Pi_Y \cap \Pi_N = \emptyset$. The set $\Pi_Y$
captures all of the ``yes'' instances, and the set $\Pi_N$ captures all of the
``no'' instances. The set $\Pi_Y \cup \Pi_N$ is called the \emph{promise}. We
say that a Turing machine $M$ \emph{solves} a promise problem $\Pi$ if on
every input $x \in \Pi_Y$ it accepts and on input $x \in \Pi_N$ it rejects.
Crucially, for inputs not in the promise, the machine can behave arbitrarily.

Promise problems generalize decision problems (languages) in the case that
$\Pi_Y \cup \Pi_N = \{0,1\}^*$, and complexity classes based on decision
problems have obvious analogs based on promise problems. For example, $\prP$ is
the class of all promise problems solvable by a deterministic polynomial-time
Turing machine. Similarly, the class $\prBPP$ is the class of promise problems
solvable by a probabilistic polynomial-time Turing machine with error
probability at most 1/3 for all instances. The class $\prBPP$ will be one of
our main objects of study, so its definition is worth emphasizing.

\begin{definition}[$\prBPP$]
  A promise problem $\Pi = (\Pi_Y, \Pi_N)$ is in $\prBPP$ if there exists a
  polynomial $p(n)$ and a probabilistic Turing machine $M$ such that
  \begin{enumerate}
    \item if $x \in \Pi_Y$, then $M$ accepts on input $x$ with probability
      greater than $2/3$ after $p(n)$ steps, and
    \item if $x \in \Pi_N$, then $M$ rejects on input $x$ with probability
      greater than $2/3$ after $p(n)$ steps.
  \end{enumerate}
\end{definition}

The constant $2/3$ in the definition of $\prBPP$ is not important; it can be
replaced by any other constant greater than 1/2 without changing the class.

The following observation states that if we can derandomize promise problems,
then we can also derandomize languages. Its proof is trivial.

\begin{observation}
  If $\prBPP = \prP$, then $\BPP = \P$.
\end{observation}

What do we know about the placement $\BPP$ with respect to other complexity
classes? An easy inclusion is $\BPP \subseteq \PSPACE$.

\begin{proposition}
  $\BPP \subseteq \PSPACE$.
\end{proposition}
\begin{proof}
  Briefly, the idea is to simulate the $\BPP$ algorithm with all possible
  random strings and count the fraction of acceptances.

  In more detail, let $L \in \BPP$ and let $M$ be a probabilistic Turing
  machine that solves $L$ in polynomial time $p(n)$. We can design a
  deterministic machine $M'$ that solves $L$ with $O(p(n))$ space as follows.
  On input $x$, $M'$ simulates $M$ on input $(x, r)$ for every $r \in
  \set{0,1}^{p(n)}$ and counts the number of times $M$ accepts. If $M$ accepts
  for more than $\frac{2}{3} \cdot 2^{p(n)}$ choices of $r$, then $M'$ accepts.
  Otherwise, $M'$ rejects.

  Clearly $M'$ accepts $L$ if and only if $M$ accepts $L$. The space
  requirements of $M'$ are the space requirements of $M$ plus the $p(n)$ bits
  for $r$ plus the $p(n)$ bits for the counter, and hence $M$ decides $L$ in
  polynomial space.
\end{proof}

Unfortunately, we cannot yet prove $\BPP \ne \PSPACE$; such a proof would imply
that $\P \ne \PSPACE$, resolving a long-standing open problem. Here is
tantalizing open problem that would not as strong consequences, and thus should
be easier to prove: Show that $\BPP \ne \NEXP$. We know that $\P
\ne \EXP$ by the time-hierarchy theorem, so we certainly expect $\BPP \ne
\EXP$, but surprisingly, the problem remains wide open.

\subsection{Equivalent Definitions of prBPP via Amplification}

Let $\prBPP[\delta(n)]$ be the class of promise problems that are solved by a
polynomial-time Turing machine with error probability at most $\delta(n)$ on
all input strings of length $n$. Notice that $\prBPP = \prBPP[1/3]$. We have
already mentioned that for all constant $1> \delta_0 > 1/2$, $\prBPP[\delta_0] =
\prBPP$. However, perhaps surprisingly, the class $\prBPP[1/2]$ appears much
more powerful than $\prBPP$: for example, $\NP \subseteq \prBPP[1/2]$.%
\footnote{The non-promise class corresponding to what we call $\prBPP[1/2]$ is
called $\PP$. The class corresponding to $\prBPP[1]$ is just $\prP$.}

Recall the following additive Chernoff bound.

\begin{theorem}[Additive Chernoff lower tail bound]\label{thm:chernoff}
  Let $X_1, \dots, X_n$ are independent random variables where $X_i \sim
  \Bern(p)$, let $X = \sum_{i=1}^n X_i$, and let $\mu = \Exp X$. For all
  $\lambda \ge 0$, \[
    \Pr\paren*{X \le \mu - \lambda} \le \exp\paren*{-\frac{\lambda^2}{2\mu}}.
  \]
\end{theorem}

\begin{proposition}[Majority trick]\label{prop:maj}
  Let $\eps \in (0, \frac{1}{2}]$, let $p \ge \frac{1}{2} + \eps$, and let $X_1,
  \dots, X_t \sim \Bern(p)$ be i.i.d. The majority of $X_1, \dots, X_t$ is 1
  with probability at least $1 - \exp(-\eps^2 t/2)$.
\end{proposition}

\begin{proof}
  Let $X = \sum_{i=1}^t X_i$ and let $\mu = \Exp X$ and observe that $\mu \ge
  t/2 + t\eps$ and $\mu \le t$. If $X > t/2$, then the majority is $1$. By the
  Chernoff bound of Theorem~\ref{thm:chernoff}, the probability that this does
  not happen is \[
    \Pr(X \le t/2) = \Pr(X \le \mu - \eps t) < \exp\paren*{-\frac{\eps^2t^2}{2\mu}} \le \exp\paren*{-\frac{\eps^2t}{2}}.
  \]
  Hence the output is 1 with probability at most $1 - \exp(-\eps^2 t/2)$.
\end{proof}

\begin{corollary}\label{cor:bpp-small-error}
  If $\delta(n)$ satisfies $\delta(n) = 2^{-\poly(n)}$ and $1/2 - \delta =
  1/\poly(n)$, then $\prBPP[\delta(n)] = \prBPP$.
\end{corollary}

\begin{proof}
  By Proposition~\ref{prop:maj}, any algorithm with success rate $\delta_0 =
  1/2 + \eps$ can be used to produce an algorithm with success rate $\delta_1 >
  \delta_0$ by repeating the algorithm independently $t =
  2\ln(1 / \delta_1)/\eps^2$ times and taking the majority. The conditions on
  $\delta_0$ and $\delta_1$ ensure that $t = \poly(n)$.
\end{proof}

\begin{corollary}
  $\BPP \subseteq \Ppoly$.
\end{corollary}

\begin{proof}
    Let $L \in \BPP$. By Corollary~\ref{cor:bpp-small-error}, there exists a
    probabilistic polynomial-time machine $M$ solving $L$ with error
    probability at most $2^{-2n}$.

    Fix $n$ and let $f_n : \{0, 1\}^n \to \{0, 1\}$ be the indicator for $L
    \cap \{0, 1\}^n$. Let $A_x$ be the event that $M(x, r) \ne f_n(x)$, where
    the randomness is over the choice of $r$. For any fixed $x \in \{0, 1\}^n$,
    we have
    \begin{align*}
      \Pr\brac*{A_x} \leq 2^{-2n}.
    \end{align*}
    Union bounding over all inputs of length $n$, \[
      \Pr\brac*{\bigcup_{x \in \set{0, 1}^n} A_x}
      \leq \sum_{x \in \set{0,1}^n} \Pr_r\brac*{A_x}
      \leq \sum_{x \in \set{0,1}^n} 2^{-2n}
      < 1.
    \]
    There is thus some $r^*$ such that $A_x$ does not occur for any $x$. In
    other words, $M(x,r^*) = f_n(x)$ for all inputs $x$ of length $n$. Hence the
    machine that on input $x$ and advice $r^*$ simulates $M$ on input $(x,
    r^*)$ solves $L$ in polynomial time using polynomial bits of advice.
\end{proof}

There are a few reasons to believe that we can do better. Consider the following propositions (informal):

\begin{proposition}[BM82,GGM84]
	If there exist one way functions secure against sub exponential size circuits then $\prBPP \subseteq \prQP = \cup_c \prDTIME[n^{\log^c n}]$.
\end{proposition}

Circuit lower bound
\begin{proposition}[IW97]
	If $\,\exists \eps>0$ such that $\DTIME[2^n]$ is hard for circuits of size $2^{\eps n}$ on almost all inputs then $\prBPP=\prP$.
\end{proposition}

\begin{proposition}
	If there exists a function computable by uniform circuits of size $\poly(n)$ and depth $n^2$ that is hard for probabilistic time $n^{100}$ on almost all inputs then $\prBPP = \prP$.
\end{proposition}

\begin{proposition}
	$\BPP \neq \P \rightarrow \P \neq \NP$.
\end{proposition}
\begin{proof}[Short proof]
This is because if $\P = \NP$ then the polynomial hierarchy collapses. We know that $\BPP \subseteq \Sigma_2$ which implies $\BPP = \P$.
\end{proof}

One strong reason to believe in derandomization is that whenever we try we are always successful. 

\subsection{The Circuit Acceptance Probability Problem (CAPP)}

The \emph{acceptance probability} of a circuit is the fraction of inputs for
which it outputs 1.

\begin{definition}[$\CAPP$]
  $\CAPP$ is the promise problem where
  \begin{enumerate}
    \item $\CAPP_Y$ is the set of all descriptions of circuits that have
      acceptance probability greater than $2/3$ over the inputs, and
    \item $\CAPP_N$ is the set of all descriptions of circuits that have
      acceptance probability greater than $1/3$.
  \end{enumerate}
\end{definition}

\begin{theorem}[$\CAPP$ is $\prBPP$-complete]\label{thm:complete}
  $\CAPP$ is complete for $\prBPP$ under deterministic polynomial-time reductions.
\end{theorem}

$\CAPP$ is a promise problem because circuits that accept exactly half their inputs neither belong to $\CAPP_Y$ nor to $\CAPP_N$. $\CAPP$ is in $\prBPP$ because given a circuit we can plug in random strings and check if most of the strings are accepted or rejected. We now show why it is complete for $\prBPP$.

Recall from the reduction in Cook's theorem that every Turing machine can be
efficiently converted to an equivalent circuit for a fixed input length.
\begin{lemma}\label{lem:tm-to-circuit}
  For all $n \in \N$ there is an $O(t^2)$-time algorithm that, on input
  $\langle M \rangle$ for some Turing machine $M$ with running time $t$, outputs a circuit $C_n$ such
  that $C_n$ accepts $x \in \{0, 1\}^n$ if and only if $x \in L(M)$.
\end{lemma}
\comment{$t$ is the running time of $M$ right?}

\begin{proof}[Proof of Theorem~\ref{thm:complete}]
  The problem $\CAPP$ is easily seen to be in $\prBPP$: consider the algorithm
  that simply simulates the input circuit on a randomly chosen input. So let us
  show that every problem in $\prBPP$ can be reduced to $\CAPP$ in
  deterministic polynomial time. Let $\Pi \in \prBPP$ and let $M$ be the
  probabilistic polynomial-time Turing machine solving $\Pi$. Given an input
  $x$, by Lemma~\ref{lem:tm-to-circuit}, we can construct in polynomial-time a
  circuit $C_x$ such that $C_x(r) = M(x, r)$ for all $r$. Hence if $x \in
  \Pi_Y$, then $C_x \in \CAPP_Y$, and if $x \in \Pi_N$, then $C_x \in \CAPP_N$.
\end{proof}

\subsection{A Time-Hierarchy Theorem}

\begin{theorem}
  $\prBPTIME[T] \subsetneq \prBPTIME[\tilde{O}(T)]$
\end{theorem}

\comment{TODO: Why standard diag fails for BPP. Why it fails for prBPP. 
         Why lazy diag works for prBPP. Why it still fails for BPP.}

We do not know such a result for $\BPP$. We can prove something similar when there are bits of advice:

\begin{proposition}
	$ \BPTIME[T]/1  \subsetneq \BPTIME[T^2]/1$ 
\end{proposition}



\subsection{Reducing Search Problems to Decision Problems}

We now want to show that problems in the search world of $\BPP$ can be reduced to decision problems in $\BPP$ (so it is okay to focus on decision problems). We start with some definitions:

\begin{definition}
	Let $\Pi = (\Pi_Y,\Pi_N)$ be a promise problem and let $R=\set{ (x,w) \subseteq \Pi_Y \times \set{0,1}^* }$ be a relation denoting pairs of yes instances and their corresponding solutions. Also, assume that there exists a polynomial $p$ such that $\forall (x,w) \in R, \, \card{w} \leq p(\card{x})$.
	
	\begin{enumerate}
		\item $\Pi \in \Psr$ if there is a polynomial time machine $M$ such that $\forall x \in \Pi_Y$ we have $(x, M(x)) \in R$ and $R \in \prP$.
		\item $\Pi \in \BPPsr$ if there is a probabilistic polynomial time machine $M$ such that $\forall x \in \Pi_Y$ we have $\Pr( (x, M(x)) \in R) \geq 1/2$ and $R \in \prBPP$.		
	\end{enumerate}
\end{definition}

$R$ is a promise problem because there are yes instances $(x,w)$ where $w$ is the correct answer to $x$, there are no instances $(x,w')$ where $w'$ is not the correct answer to $x$, and there are instances that do not satisfy the promise $(x',w)$ where $x'$ is not a yes instance of $\Pi$ hence has no solution.

\begin{theorem}
	If $\prBPP = \prP$ then $\BPPsr = \Psr$.
\end{theorem}
\begin{proof}
	Assume $\prBPP=\prP$. Given $x$ we reduce the problem of finding a solution $w$ such that $(x,w) \in R$ to a decision problem in $\prBPP$. Let $M$ be the probabilistic polynomial time machine that finds solutions. Let $M_R$ be the probabilistic polynomial time machine that decides $R$ and let $D_R$ be the deterministic polynomial time machine that decides $R$ (such a machine exists since $\prBPP=\prP$). 
	Let $C_x(r)= D_R(x, M(x,r))$ be the circuit that has the input $x$ hard coded and gets random coins $r$ and runs $M$ with $r$ and tests whether the solution is correct. We know $\Pr_r(C_x(r)=1) \geq 1/2$.
	
	We now want to construct good random coins for $M$ ($\bar{r}$ st $M(x,\bar{r})$ is a solution). Denote the runtime of $M$ by $T$ (the number of random bits used by $M$ is thus bounded by $T$). We will work in $T$ iterations and in each iteration we will fix one of the random bits and finally end up with $\bar{r}$. Let $\bar{r}_0 = \lambda, \card{\bar{r}_i}=i, \card{\bar{r}_n} = \card{\bar{r}} = T$.
	For $i=1$ to $T$ maintain the following invariant:
	\[
		\Pr_{b_{i+1},\ldots b_T}[C_x(\bar{r}_i, b_{i+1} \ldots b_T))]
	\]
	
	
\end{proof}

\subsection{Two-Sided Error vs.\ One-Sided Error}

\comment{TODO: reference Lecture 5, in which the theorem below is proved}

\begin{definition}\label{def:sampler}
  An \emph{$(\eps, \delta)$-sampler} is a function $s :
  \set{0,1}^{\overline{T}} \times \set{0,1}^\ell \to \set{0,1}^T$ such that for
  all $f : \set{0,1}^T \to \set{0,1}$ with probability at least $1 - \delta$ 
  over $z \in \set{0,1}^{\overline{T}}$ it holds that \[
    |\Exp_i f(s(z, i)) - \Exp_x f(x)| \le \eps.
  \]
\end{definition}

In \Cref{def:sampler}, $\set{0,1}^{\overline{T}}$ should be thought of as a random input and $\set{0,1}^\ell$ should be thought of as an index. This definition is then saying that for any boolean test $f$, for most strings ($1-\delta$ fraction), the average of $f$ in the sample set (of size $2^{\ell}$) is roughly the same as the average of $f$ in the universe (of size $2^T$).

One can think of $\set{0,1}^n$ as the $2^n$ students at Rutgers and $f$ as a test of height more than $6$ feet. The sampler uses $\overline{T}$ random bits to produce a sample set of $2^\ell$ students who with good probability roughly represent the entire population at Rutgers in terms of height more than $6$ feet.

The naive sampler uses fresh randomness for each sample so $\overline{T}$ is a random string of length $2^\ell \cdot n$.

\begin{theorem}
  For all $\eps > 0$ there exists a polynomial-time computable $(\eps,
  \delta)$-sampler with $\overline{T} = \poly(T)$, $\ell = O(\log
  \overline{T}/\eps)$ and $\delta = 2^{\overline{T}^\eps - \overline{T}}$.
\end{theorem}

\begin{corollary}\label{cor:bpp-small-random}
  For all $\eps > 0$, the class $\prBPP$ can be equivalently defined with an
  error of $2^{T^\eps - T}$ instead of 1/3, where $T = T(n)$ is the number of
  random bits.

  In other words, $2^{T^\eps}$ out of the $2^T$ random bit-strings are exceptional.
\end{corollary}

\begin{theorem}\label{thm:bpp-subset-rprp}
    $\prBPP \subseteq \prRP^\prRP$.
\end{theorem}

\begin{proofsk}
    Let $\Gamma = (\Gamma_Y, \Gamma_N)$ be the following promise problem:
    \begin{itemize}
        \item $\Gamma_Y = \set{\langle C \rangle \mid \Pr\brac{C(r) = 1} = 1}$,
        \item $\Gamma_N = \set{\langle C \rangle \mid \Pr\brac{C(r) = 0} \geq 1 - 2^{\sqrt{|r|} - |r|}}$.
    \end{itemize}
    One can verify that $\Gamma$ is indeed in $\prcoRP$.

    Let $\Pi = (\Pi_Y, \Pi_N) \in \prBPP$.
    By Corollary~\ref{cor:bpp-small-random}, there is a probabilistic polynomial time 
    Turing Machine $M$ deciding $x \in \Pi_Y$ or $x \in \Pi_N$ with $2^{\sqrt{p(|x|)} 
    - p(|x|)}$ error probability over $p(|x|)$ random bits.
    
    Consider the following $\prRP^\Gamma$ algorithm that, given $x \in \Pi_Y \cup \Pi_N$, 
    decides if $x \in \Pi_Y$ or $x \in \Pi_N$.
    \begin{itemize}
        \item Compile $M(x,r)$ to a circuit $C_x(r)$ which accepts or rejects at most 
            $2^{\sqrt{|r|}}$ of its inputs (see Lemma~\ref{lem:tm-to-circuit}, 
            Theorem~\ref{thm:complete}).
        \item Fixes the first $|r|/2$ inputs of $C_x(r)$ randomly, yielding the circuit 
            $C'_x(r')$.
        \item Queries the $\Gamma$ oracle with a description of $C'_x(r')$ and returns 
            the same answer.
    \end{itemize}

    \comment{TODO: Finish up and turn into proper proof. 
    Show $\Pi_Y$ gets $1$ whp, and $\Pi_N$ gets $0$ all the time.}
\end{proofsk}

\begin{corollary}
    $\prBPP = \prP \iff \prRP = \prP$.
\end{corollary}

\begin{proof}
    If $\prBPP = \prP$, then it is clear that $\prRP = \prP$ since $\prP \subseteq
    \prRP \subseteq \prBPP$.

    Conversely, if $\prRP = \prP$, then by Theorem~\ref{thm:bpp-subset-rprp},
    $\prBPP \subseteq \prRP^\prRP = \prP^\prP = \prP$.
    Since $\prP \subseteq \prBPP$, we have $\prBPP = \prP$.
\end{proof}

\end{lecture}
