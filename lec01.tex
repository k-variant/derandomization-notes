\begin{lecture}{1}{The Class prBPP}{January 23, 2020}
\label{lec:01}

\subsection*{The Upshot}

\begin{enumerate}
  \item Randomness is necessary in many computational settings. Is randomness
    necessary for polynomial-time computation? There are good reasons to expect
    it is not! This is a fundamental question we will investigate in this
    course.
  \item Promise problems are a natural generalization of languages, and the
    theory of probabilistic algorithms for promise problems is richer than
    for that of languages.
  \item The class $\prBPP$ has a complete problem ($\CAPP$) under deterministic
    polynomial-time reductions. Hence to prove $\prBPP = \prP$ it suffices
    to give a deterministic polynomial-time algorithm for $\CAPP$.
  \item The class $\prBPP$ admits a time-hierarchy theorem.
  \item Probabilistic search problems can be reduced to probabilistic decision
    problems, meaning the theory we will develop around the class of decision
    problems $\prBPP$ will apply to the search analog as well.
\end{enumerate}

\subsection{Promise Problems and prBPP}

A \emph{promise problem} is a pair $\Pi = (\Pi_Y, \Pi_N) \subseteq \{0, 1\}^*
\times \{0, 1\}^*$ such that $\Pi_Y \cap \Pi_N = \emptyset$. The set $\Pi_Y$
captures all of the ``yes'' instances, and the set $\Pi_N$ captures all of the
``no'' instances. The set $\Pi_Y \cup \Pi_N$ is called the \emph{promise}. We
say that a Turing machine $M$ \emph{solves} a promise problem $\Pi$ if on
input $x \in Pi_Y$ it accepts and on input $x \in \Pi_N$ it rejects. Crucially,
for inputs not in the promise, the machine can behave arbitrarily.

Promise problems generalize decision problems (languages) in the case that
$\Pi_Y \cup \Pi_N = \Pi$. Complexity classes based on decision problems have
obvious analogs based on promise problems. For example, $\prP$ is the class of
all promise problems solvable by a deterministic polynomial-time Turing
machine. Similarly, the class $\prBPP$ is the set of decision problems solvable
by a probabilistic polynomial-time Turing machine with error probability at
most 1/3 for all instances. The class $\prBPP$ will be one of our main objects
of study, so its definition is worth repeating carefully.

\begin{definition}[$\prBPP$]
  A promise problem $\Pi = (\Pi_Y, \Pi_N)$ is in $\prBPP$ if and only if there
  exists polynomial $p(n)$ and a probabilistic Turing machine $M$ such that
  \begin{enumerate}
    \item if $x \in \Pi_Y$, then $M$ accepts input $x$ with probability at
      least $2/3$ after $p(n)$ steps, and
    \item if $x \in \Pi_N$, then $M$ rejects input $x$ with probability at
      least $2/3$ after $p(n)$ steps.
  \end{enumerate}
\end{definition}

The constant $2/3$ in the definition of $\prBPP$ is not consequential; it
can be replaced by any other constant greater than 1/2, which we will discuss
in more detail in the next section.%
\footnote{However, if the success probability is just ``strictly greater than
  1/2,'' then we obtain the class \PP, which contains, in particular, \NP and
  therefore is believed to be a strict superset of \BPP.}

\begin{proposition}
  If $\prBPP = \prP$, then $\BPP = \P$.
\end{proposition}

\begin{proof}
  If $\Pi \in \BPP$, then trivially $\Pi \in \prBPP$ and therefore by
  assumpiton $\Pi \in \prP$. As $\Pi$ is a decision problem, it follows
  that $\Pi \in \P$.
\end{proof}

What do we know about the placement $\BPP$ with respect to other complexity
classes? A trivial inclusion is $\BPP \subseteq \PSPACE \subseteq \EXP
\subseteq \NEXP$. Open problem: prove $\BPP \ne \NEXP$.

Using the amplification techniques of the next section, one can also prove
the following theorem.

\begin{theorem}
  $\BPP \subseteq \Ppoly$.
\end{theorem}


\subsection{Equivalent Definitions of prBPP via Amplification}

\begin{definition}
  An \emph{$(\eps, \delta)$-sampler} is a function $s :
  \set{0,1}^{\overline{T}} \times \set{0,1}^\ell \to \set{0,1}^T$ such that for
  all $f : \set{0,1}^T \to \set{0,1}$ and for all $z \in
  \set{0,1}^{\overline{T}}$ it holds that \[
    |\Exp_i f(s(z, i)) - \Exp_x f(x)| \le \eps.
  \]
\end{definition}

\begin{theorem}
  For all $\eps > 0$ there exists a polynomial-time computable $(\eps,
  \delta)$-sampler with $\overline{T} = \poly(T)$, $\ell = O(\log
  \overline{T}/\eps)$ and $\delta = 2^{\overline{T}^\eps - \overline{T}}$.
\end{theorem}

\begin{corollary}
  The class $\prBPP$ can be equivalently defined with an error of $2^{T^\eps -
  T}$ instead of 1/3, where $T = T(n)$ is the number of random bits.
\end{corollary}

\subsection{The Circuit Acceptance Probability Problem (CAPP)}

The \emph{acceptance probability} of a circuit is the fraction of inputs for
which it outputs 1.

\begin{definition}[$\CAPP$]
  $\CAPP$ is the promise problem where
  \begin{enumerate}
    \item $\CAPP_Y$ is the set of all descriptions of circuits that have
      acceptance probability at least 2/3, and
    \item $\CAPP_N$ is the set of all descriptions of circuits that have
      acceptance probability at most 1/3.
  \end{enumerate}
\end{definition}

\begin{theorem}[$\CAPP$ is $\prBPP$-complete]
  $\CAPP$ is complete for $\prBPP$ under deterministic polynomial-time reductions.
\end{theorem}

\comment{State the form of the Cook-Levin theorem we need in the following proof.}

\begin{proof}
  The problem $\CAPP$ is easily seen to be in $\prBPP$: consider the algorithm
  that simply simulate the input circuit on a randomly chosen input.

  Now we argue that every problem in $\prBPP$ can be reduced to $\CAPP$ in
  polynomial-time. Let $\Pi \in \prBPP$ and let $M$ be the probabilistic
  polynomial-time Turing machine solving $\Pi$. {\`A} la Cook-Levin, for every
  input $x$, we can construct in polynomial-time a circuit $C_x$ such that
  $C_x(r) = M(x, r)$. Hence if $x \in \Pi_Y$, then $C_x \in \CAPP_Y$, and if $x
  \in \Pi_N$, then $C_x \in \CAPP_N$.
\end{proof}

\subsection{A Time-Hierarchy Theorem}

\begin{theorem}
  $\prBPTIME[T \log{T}] \subsetneq \prBPTIME[T]$
\end{theorem}

\subsection{Reducing Search Problems to Decision Problems}

\subsection{Two-Sided Error vs.\ One-Sided Error}

\end{lecture}
