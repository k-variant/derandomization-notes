\begin{lecture}{1}{The Class prBPP}{January 23, 2020}
\label{lec:01}

\subsection*{The Upshot}

\begin{enumerate}
  \item Randomness is necessary in many computational settings. Is randomness
    necessary for polynomial-time computation? There are good reasons to expect
    it is not! This is a fundamental question we will investigate in this
    course.
  \item Promise problems are a natural generalization of languages, and the
    theory of probabilistic algorithms for promise problems is richer than
    for that of languages.
  \item The class $\prBPP$ has a complete problem ($\CAPP$) under deterministic
    polynomial-time reductions. Hence to prove $\prBPP = \prP$ it suffices
    to give a deterministic polynomial-time algorithm for $\CAPP$.
  \item The class $\prBPP$ admits a time-hierarchy theorem.
  \item Probabilistic search problems can be reduced to probabilistic decision
    problems, meaning the theory we will develop around the class of decision
    problems $\prBPP$ will apply to the search analog as well.
\end{enumerate}

\subsection{Promise Problems and prBPP}

A \emph{promise problem} is a pair $\Pi = (\Pi_Y, \Pi_N) \subseteq \{0, 1\}^*
\times \{0, 1\}^*$ such that $\Pi_Y \cap \Pi_N = \emptyset$. The set $\Pi_Y$
captures all of the ``yes'' instances, and the set $\Pi_N$ captures all of the
``no'' instances. The set $\Pi_Y \cup \Pi_N$ is called the \emph{promise}. We
say that a Turing machine $M$ \emph{solves} a promise problem $\Pi$ if on
input $x \in \Pi_Y$ it accepts and on input $x \in \Pi_N$ it rejects. Crucially,
for inputs not in the promise, the machine can behave arbitrarily.

Promise problems generalize decision problems (languages) in the case that
$\Pi_Y \cup \Pi_N = \{0,1\}^*$. Complexity classes based on decision problems
have obvious analogs based on promise problems. For example, $\prP$ is the
class of all promise problems solvable by a deterministic polynomial-time
Turing machine. Similarly, the class $\prBPP$ is the class of promise problems
solvable by a probabilistic polynomial-time Turing machine with error
probability at most 1/3 for all instances. The class $\prBPP$ will be one of
our main objects of study, so its definition is worth repeating carefully.

\begin{definition}[$\prBPP$]
  A promise problem $\Pi = (\Pi_Y, \Pi_N)$ is in $\prBPP$ if and only if there
  exists a polynomial $p(n)$ and a probabilistic Turing machine $M$ such that
  \begin{enumerate}
    \item if $x \in \Pi_Y$, then $M$ accepts input $x$ with probability greater
      than $2/3$ after $p(n)$ steps, and
    \item if $x \in \Pi_N$, then $M$ rejects input $x$ with probability greater
      than $2/3$ after $p(n)$ steps.
  \end{enumerate}
\end{definition}

The constant $2/3$ in the definition of $\prBPP$ is not important; it
can be replaced by any other constant greater than 1/2, which we will discuss
in more detail in the next section.%

\begin{proposition}
  If $\prBPP = \prP$, then $\BPP = \P$.
\end{proposition}

\begin{proof}
  If $\Pi \in \BPP$, then trivially $\Pi \in \prBPP$, and therefore by
  assumption, $\Pi \in \prP$. As $\Pi$ is a decision problem, it follows
  that $\Pi \in \P$.
\end{proof}

What do we know about the placement $\BPP$ with respect to other complexity
classes? An easy inclusion is $\BPP \subseteq \PSPACE$.

\begin{proposition}
  $\BPP \subseteq \PSPACE$.
\end{proposition}
\begin{proof}
  In one sentence: Simulate the $\BPP$ algorithm with all possible random
  strings.

  In more detail, let $L \in \BPP$ and let $M$ be a probabilistic Turing
  machine that solves $L$ in polynomial time $p(n)$. We can design a
  deterministic machine $M'$ that solves $L$ as follows. On input $x$, $M'$
  simulates $M$ on input $(x, r)$ for every $r \in \set{0,1}^{p(n)}$ and counts
  the number of times $M$ accepts. If $M$ accepts for more than $\frac{2}{3}
  \cdot 2^{p(n)}$ choices of $r$, then $M'$ accepts. Otherwise, $M'$ rejects.

  Clearly $M'$ accepts $L$ if and only if $M$ accepts $L$. The space
  requirements of $M'$ are the space requirements of $M$ plus the $p(n)$ bits
  for $r$ plus the $p(n)$ bits for the counter, and hence $M$ decides $L$ in
  polynomial space.
\end{proof}

Of course we cannot yet prove $\BPP \ne \PSPACE$ because we cannot yet prove
$\P \ne \PSPACE$. The deterministic time hierarchy theorem tells us that $P \ne
\EXP$, but suprisingly, it is still only a conjecture that $\BPP \ne \NEXP$!

\subsection{Equivalent Definitions of prBPP via Amplification}

Let $\prBPP[\delta(n)]$ be the class of promise problems that are solved by a
polynomial-time Turing machine with error probability at most $\delta(n)$ on
all input strings of length $n$. Notice that $\prBPP = \prBPP[1/3]$. We have
already mentioned that for all constant $\delta_0 > 1/2$, $\prBPP[\delta_0] =
\prBPP$. However, perhaps surprisingly, the class $\prBPP[1/2]$ appears much
more powerful than $\prBPP$: for example, $\NP \subseteq \prBPP[1/2]$.%
\footnote{The non-promise class corresponding to what we call $\prBPP[1/2]$ is
called $\PP$.} However, we can still have $\prBPP[\delta(n)] = \prBPP$ for
$\delta(n)$ tending to $1/2$, provided $\delta$ it doesn't converge too
quickly.

\begin{proposition}
  If $\delta(n) \le 1/2 - \eps(n)$ for some function $\eps(n)$ satisfying
  $1/\eps(n) = \poly(n)$, then $\prBPP[\delta(n)] = \prBPP$.
\end{proposition}

\begin{proof}
  We may assume without loss of generality that $\delta(n) \ge 1/3$. It follows
  by definition that $\prBPP \subseteq \prBPP[\delta(n)]$. Let $\Pi \in
  \prBPP[\delta(n)]$ and let $M$ be the corresponding machine for $\Pi$. We
  must amplify the success rate of $M$ from $1/2 + \eps$ to $2/3$, and we do so
  in the obvious way: run $M$ independently many times and output the majority
  bit.

  Consider the machine $M'$ that runs $M$ independently $t := 2\ln(3)/\eps^2$
  times, records the outputs $X_1, \dots, X_t$, and returns the majority of the
  $X_i$'s. Clearly $M'$ runs in polynomial time as $1/\eps = \poly(n)$. We
  claim that $M'$ errs with probability at most 1/3. The majority of the
  $X_i$'s is wrong when $\sum_i X_i < t/2$. Let $X = \sum_{i=1}^t X_i$ and
  observe that $\Exp{X} \ge t(1/2 - \eps)$. By a Chernoff bound, the
  probability of failure is \[
    \Pr(X < t/2) = \Pr(X < \Exp{X} + \eps t) < \exp\paren*{-\eps^2 t/2} < 1/3,
  \]
  as desired.
\end{proof}

In the other direction, how small can the error be so that we can still prove
an equivalence to $\prBPP$? Obviously we cannot yet prove $\prBPP[0] = \prBPP$,
for $\prBPP[0] = \prP$.

\comment{TODO: Flesh out below a bit more and reference Lecture 5, in which the theorem below is proved}

\begin{definition}
  An \emph{$(\eps, \delta)$-sampler} is a function $s :
  \set{0,1}^{\overline{T}} \times \set{0,1}^\ell \to \set{0,1}^T$ such that for
  all $f : \set{0,1}^T \to \set{0,1}$ and for all $z \in
  \set{0,1}^{\overline{T}}$ it holds that \[
    |\Exp_i f(s(z, i)) - \Exp_x f(x)| \le \eps.
  \]
\end{definition}

\begin{theorem}
  For all $\eps > 0$ there exists a polynomial-time computable $(\eps,
  \delta)$-sampler with $\overline{T} = \poly(T)$, $\ell = O(\log
  \overline{T}/\eps)$ and $\delta = 2^{\overline{T}^\eps - \overline{T}}$.
\end{theorem}

\comment{TODO: rewrite corollary below in terms of $\prBPP[\delta]$. also write a proof}

\begin{corollary}\label{cor:bpp-small-error}
  For all $\eps > 0$, the class $\prBPP$ can be equivalently defined with an
  error of $2^{T^\eps - T}$ instead of 1/3, where $T = T(n)$ is the number of
  random bits.
\end{corollary}

\begin{corollary}
  $\BPP \subseteq \Ppoly$.
\end{corollary}

\begin{proof}
    Let $L \in \BPP$. By Corollary~\ref{cor:bpp-small-error}, there exists a
    probabilistic polynomial-time machine $M$ solving $L$ with error
    probability at most $2^{-2n}$.

    Fix $n$ and let $f_n : \{0, 1\}^n \to \{0, 1\}$ be the indicator for $L
    \cap \{0, 1\}^n$. Let $A_x$ be the event that $M(x, r) \ne f_n(x)$, where
    the randomness is over the choice of $r$. For any fixed $x \in \{0, 1\}^n$,
    we have
    \begin{align*}
      \Pr\brac*{A_x} \leq 2^{-2n}.
    \end{align*}
    Union bounding over all inputs of size $n$, \[
      \Pr\brac*{\bigcup_{x \in \set{0, 1}^n} A_x}
      \leq \sum_{x \in \set{0,1}^n} \Pr_r\brac*{A_x}
      \leq \sum_{x \in \set{0,1}^n} 2^{-2n}
      < 1.
    \]
    There is thus some $r^*$ such that $A_x$ does not occur for any $x$. In
    other words, $M(x,r^*) = f_n(x)$ for all inputs $x$ of size $n$. Hence the
    machine that on input $x$ and advice $r^*$ simulates $M$ on input $(x,
    r^*)$ solves $L$ in polynomial time using polynomial bits of advice.
\end{proof}


\subsection{The Circuit Acceptance Probability Problem (CAPP)}

The \emph{acceptance probability} of a circuit is the fraction of inputs for
which it outputs 1.

\begin{definition}[$\CAPP$]
  $\CAPP$ is the promise problem where
  \begin{enumerate}
    \item $\CAPP_Y$ is the set of all descriptions of circuits that have
      acceptance probability at least 2/3, and
    \item $\CAPP_N$ is the set of all descriptions of circuits that have
      acceptance probability at most 1/3.
  \end{enumerate}
\end{definition}

\begin{theorem}[$\CAPP$ is $\prBPP$-complete]\label{thm:complete}
  $\CAPP$ is complete for $\prBPP$ under deterministic polynomial-time reductions.
\end{theorem}

Recall from the reduction in Cook's theorem that every Turing machine can be
efficiently converted to an equivalent circuit for a fixed input length.
\begin{lemma}\label{lem:tm-to-circuit}
  For all $n \in \N$ there is an $O(t^2)$-time algorithm that, on input
  $\langle M \rangle$ for some Turing machine $M$, outputs a circuit $C_n$ such
  that $C_n$ accepts $x \in \{0, 1\}^n$ if and only if $x \in L(M)$.
\end{lemma}

\begin{proof}[Proof of Theorem~\ref{thm:complete}]
  The problem $\CAPP$ is easily seen to be in $\prBPP$: consider the algorithm
  that simply simulates the input circuit on a randomly chosen input. So let us
  show that every problem in $\prBPP$ can be reduced to $\CAPP$ in
  deterministic polynomial time. Let $\Pi \in \prBPP$ and let $M$ be the
  probabilistic polynomial-time Turing machine solving $\Pi$. Given an input
  $x$, by Lemma~\ref{lem:tm-to-circuit}, we can construct in polynomial-time a
  circuit $C_x$ such that $C_x(r) = M(x, r)$ for all $r$. Hence if $x \in
  \Pi_Y$, then $C_x \in \CAPP_Y$, and if $x \in \Pi_N$, then $C_x \in \CAPP_N$.
\end{proof}

\subsection{A Time-Hierarchy Theorem}

\begin{theorem}
  $\prBPTIME[T \log{T}] \subsetneq \prBPTIME[T]$
\end{theorem}

\comment{TODO: Why standard diag fails for BPP. Why it fails for prBPP. Why lazy diag works for prBPP. Why it still fails for BPP.}

\subsection{Reducing Search Problems to Decision Problems}

\subsection{Two-Sided Error vs.\ One-Sided Error}

\end{lecture}
