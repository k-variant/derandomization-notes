\chapter{Hardness vs.\ Randomness}
\label{lec:02}

\section{Pseudorandomness}

\section{Hitting Set Generators}

\begin{definition}[Hitting set]
  Let $f_n : \set{0, 1}^n \to \set{0, 1}$.
  A set $S \in \set{0, 1}^{n}$ is a \emph{hitting set} for $f_n$ if there exists an $r \in S$ such that $f_n(r) = 1$.
\end{definition}

\begin{definition}[Hitting set generator]
  Let $f_n : \set{0, 1}^n \to \set{0, 1}$ for $n \in \N$ and let $f = (f_n)_n$.
  A \emph{hitting set generator $H$ with seed length $\ell(n)$} is a deterministic algorithm that on input $\langle 1^n, s \rangle$, where $s \in \set{0, 1}^{\ell(n)}$, outputs a string in $\set{0, 1}^n$ such that for all $n$, \[
    \{ H(\langle 1^n, s \rangle) : s \in \set{0, 1}^{\ell(n)} \}
  \] is a hitting set for $f_n$.
\end{definition}

\begin{theorem}
    HSGs for circuits of size $\widetilde{O}(n)$ that accept ``most'' of their inputs 
    suffice to derandomize $\prRP$.
\end{theorem}

\comment{Formalize ``most''.}

\begin{definition}[Kolmogorov Complexity]
    The Kolmogorov Complexity of $x \in \set{0, 1}^*$ is the length of the shortest 
    program that, when run, prints $x$ and halts.
\end{definition}
\commenta{Zach}{Technically we need to fix a machine to define Kolmogorov complexity. Maybe something about Kolmogorov complexity belongs in an appendix.}

\begin{theorem}
    There is an algorithm $H$ that, when given $1^n$ and oracle access to a hard 
    string $f \in \set{0, 1}^{n^2}$ with Kolmogorov Complexity $|f| - o(1)$, is 
    an HSG with seed length $\ell(n) = \log{n}$ and polynomial runtime for circuits 
    of size $\widetilde{O}(n)$ that reject at most $2^{\sqrt{n}}$ inputs.
\end{theorem}

\begin{proofsk}
    Let $f = f_1 \cdot f_2 \cdot \ldots \cdot f_n$ where $|f_s| = n$ and $\cdot$ is the 
    concatenation operator.
    Define $H(1^n, \langle s \rangle) = f_s$ where $s \in [n]$.
    We claim that $H$ is an HSG with seed length $\ell(n) = \log{n}$ and polynomial 
    runtime for circuits of size $\widetilde{O}(n)$ that reject at most $2^{\sqrt{n}}$ 
    inputs.

    Assume for a contraction that $D$ is a circuit of size $\widetilde{O}(n)$ that 
    rejects at most $2^{\sqrt{n}}$ inputs and $D(H(1^n, \langle s \rangle)) = 0$ for 
    all $s \in [n]$.
    Let $i(s)$ be the index of $f_s$ in $D^{-1}(0)$.
    The following is a short description of $f$: $D, i(1), i(2), \dots, i(n)$.
    Since $\card{D^{-1}(0)} \leq 2^{\sqrt{n}}$, we have $|i(s)| \leq \sqrt{n}$.
    The length of the description is thus $\widetilde{O}(n) + n \cdot \sqrt{n}$ which 
    contradicts our assumption that the shortest description of $f$ has length $n^2 - 
    o(1)$.

    \comment{TODO?: More formal description of the program that prints $f$}
\end{proofsk}

\section{Pseudorandom Generators}

\begin{enumerate}
  \item Define $\eps$-PRG with advantage and running time.
  \item Show that the existence of a $(1/n)$-PRG for circuits of size $\widetilde{O}(n)$ with advantage $1/n^2$ and running time $\poly(n)$ implies that $\prBPP = \prP$.
\end{enumerate}

\section{The Nisan-Wigderson Generator}

\begin{theorem}
  There exists an algorithm $G$ that gets $1^n$ and oracle $f : \set{0, 1}^\ell \to \set{0, 1}$ where $\ell = O(\log{n})$ such that for all circuits $C : \set{0, 1}^\ell \to \set{0, 1}$ of size $2^{\ell/10}$ \[
    \Pr_{x \sim \set{0, 1}^n}[C(x) = f(x)] \le \nicefrac{1}{2} + \nicefrac{1}{n^2}
  \]
  is a $(1/n)$-PRG with runtime $\poly(n)$ for circuits of size $\widetilde{O}(n)$.
\end{theorem}

\begin{corollary}
  If there is a language $L \in \DTIME[2^{O(n)}]$ such that
  every circuit of size $2^{n/10}$ computes $L$ with advantage at most $1/n^2$, then $\prBPP = \prP$.
\end{corollary}

\begin{definition}[Combinatorial design]\label{def:design}
  A collection of subsets $T_1, \dots, T_m \subseteq [d]$ is a \emph{$(\ell, a)$-design}
	\begin{enumerate}
    \item $\card{T_i} = \ell$ for all $i$, and
		\item $\card{T_i \cap T_j} < a$ for all $i \ne j$.
	\end{enumerate}
\end{definition}

We will be interested in combinatorial designs where $m = 2^\ell$, $d =
O(\ell)$, and $a = \gamma \ell$ for some constant $\gamma > 0$. The following
proposition states that, not only does such a design exist for all $\ell$ and
all $\gamma$, but it can be computed deterministically in time $O(2^\ell)$.

\begin{theorem}\label{thm:design}
  Let $\gamma > 0 $ and let $\ell, m \in \mathbb{N}$. For all $a \ge \gamma
  \log_2{m}$ and $d \ge e^2 \cdot 2^{1/\gamma} \cdot \ell^2/a$, there exists an
  $(\ell, a)$-design $T_1, \dots, T_m \subseteq [d]$. Moreover, such a design
  can be found deterministically in time $\poly(m, d)$.
\end{theorem}

The proof of the theorem will use the following lemma.
\begin{lemma}\label{lem:design}
  Let $a, \ell, d \in \N$ with $a \le \ell \le d$, and let $T_1, \dots, T_m \in
  \binom{[d]}{\ell}$. If $m < \binom{d}{a} / \binom{\ell}{a}^2$, then there
  exists a set $T^* \in \binom{[d]}{\ell}$ such that $\card{T^* \cap T_i} < a$
  for all $i \in [m]$. Moreover, such a set $T^*$ can be found
  deterministically in time $\poly(m, d)$.
\end{lemma}

\commenta{Zach}{We should probably move this proof to an appendix or make it an exercise, as this is not really our business.}

\begin{proof}
  We first use the probabilistic method to show that such a set $T^*$ exists.
  Let $\mathbb{T}$ be a uniformly random $\ell$-set over $[d]$, let $X_j$
  be an indicator for the ``bad'' event $B_j$ that $|\mathbb{T} \cap T_j| \ge
  a$, and let $X = \sum_{j=1}^m X$ be the number of bad events. It suffices to
  show that \[
    \Exp\brac*{X} < 1,
  \]
  because then there is some realization of $\mathbb{T}$ for which all $X_j$
  are 0.

  The number of $\ell$-sets whose intersection with $T_j$ is at least $a$ is
  most $\binom{\ell}{a}\binom{d - a}{\ell - a}$: first choose $a$ elements from
  $T_j$ and then pick any $\ell - a$ elements from the remaining elements. Hence
  \[
    \Pr(B_j) \le \frac{\binom{\ell}{a}\binom{d - a}{\ell - a}}{\binom{d}{\ell}} =
    \frac{\binom{\ell}{a}^2}{\binom{d}{a}},
  \]
  where we used the identity $\binom{d}{\ell}\binom{\ell}{a} =
  \binom{d}{a}\binom{d - a}{\ell - a}$ in the last step. If $m < \binom{d}{a} /
  \binom{\ell}{a}^2$, then \[
    \Exp\brac*{X} =
    \sum_{j=1}^m \Pr(B_j) \le m \cdot \frac{\binom{\ell}{a}^2}{\binom{d}{a}} < 1,
  \]
  as desired.

  Now we describe how to construct such a set $T^*$ deterministically using the
  \emph{method of conditional probabilities}. At a high level, the algorithm
  simply iterates over the elements $i \in [d]$ and in each iteration decides
  whether to include $i$ in $T^*$ based on a conditional expectation
  calculation. For example, suppose we want to decide if the element $1$ should
  be included. We have \[
    1 > \Exp\brac*{X} = \Pr(1 \in \mathbb{T}) \cdot \Exp\brac*{X \mid 1 \in \mathbb{T}} 
      + \Pr(1 \not\in \mathbb{T}) \cdot \Exp\brac*{X \mid 1 \not\in \mathbb{T}},
  \]
  and so one of $\Exp\brac{X \mid 1 \in \mathbb{T}}$ or $\Exp\brac{X \mid 1 \not\in
  \mathbb{T}}$ is less than one. If $\Exp\brac{X \mid 1 \in \mathbb{T}} < 1$,
  then it is safe to add $1$ to $T^*$. Inductively, when we consider adding
  element $i$ to $\mathbb{T}$, one of the two expectations will be less than
  one, which gives us a decision for $i$.

  The only thing left to show is that for any set $T \subseteq [i]$, we can
  compute $\Pr(B_j \mid \mathbb{T} \cap [i] = T)$ exactly. This is a bit of an
  ugly calculation, but only involves basic combinatorics; we leave the details
  to the reader.
\end{proof}

Now we return to the proof of Theorem~\ref{thm:design}.

\begin{proof}[Proof of Theorem~\ref{thm:design}]
  A routine calculation shows that for $a \ge \gamma \log_2{m}$ and $d \ge e^2
  \cdot 2^{1/\gamma} \cdot \ell^2/a$, we have $m \le \binom{d}{a} /
  \binom{\ell}{a}^2$. Indeed,
  \begin{align*}
    \binom{d}{a} \cdot \binom{\ell}{a}^{-2}
    &\ge \paren*{\frac{d}{a}}^a \cdot \paren*{\frac{e \ell}{a}}^{-2a}\\
    &\ge \paren*{\frac{e^2 \cdot 2^{1/\gamma} \cdot \ell^2}{a^2}}^{a} \cdot \paren*{\frac{e^2 \ell^2}{a^2}}^{-a}\\
    &= 2^{a / \gamma}\\
    &\ge m.
  \end{align*}
  Hence we can apply Lemma~\ref{lem:design} iteratively $m$ times to obtain the
  sets $T_1, \dots, T_m$.
\end{proof}

\section*{The Upshot}

\begin{enumerate}
  \item Hardness and randomness are intimately connected. Suitably hard
    functions $f$ can be used to build PRGs that exponentially stretch random
    seeds into a string that appears random to circuits that cannot compute $f$
    with any significant advantage.
  \item The quintisential construction of such a PRG is the Nisan-Wigderson PRG
    (NW-PRG).
  \item The NW-PRG applies the hard function $f$ to many mostly disjoint
    projections of the seed; the projections are given by a combinatorial
    design.
  \item The NW-PRG shows that if there is constant $\varepsilon > 0$ and a
    language $L \in \DTIME[2^{O(n)}]$ that is hard for circuits of size
    $2^{\varepsilon n}$, then there is a $(1/n)$-PRG for circuits of size
    $\widetilde{O}(n)$. In particular, the existence of such a $\varepsilon$ and
    $L$ implies $\prBPP = \prP$.
  \item A key technique in the proof of the previous item is the \emph{hybrid
    argument}, which asserts that if a circuit $D$ distinguishes the uniform
    distribution from another product distribution, then it distinguishes two
    ``hybrid'' product distributions whose marginal distributions are the same
    in all but one coordinate.
\end{enumerate}
